{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lee-53/ECE-4380-F25/blob/main/Clean_Classification_and_performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification and Performance"
      ],
      "metadata": {
        "id": "rlkfGa7Vlliw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure you are connected to a T4 GPU runtime. The following code should report true if you are."
      ],
      "metadata": {
        "id": "ohH8QbmKmW_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "print(torchvision.__version__)\n",
        "\n",
        "\n",
        "import torch\n",
        "print(\"GPU available =\", torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "yETM8zfLvxND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install prerequisites needed for this assignment, `thop` is used for profiling PyTorch models https://github.com/ultralytics/thop, while `tqdm` makes your loops show a progress bar https://tqdm.github.io/"
      ],
      "metadata": {
        "id": "eU2jaYMIlrKg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGXNOmrkle7c"
      },
      "outputs": [],
      "source": [
        "!pip install thop segmentation-models-pytorch transformers\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import gc\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "import segmentation_models_pytorch as smp\n",
        "import thop\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# we won't be doing any training here, so let's disable autograd\n",
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Classification\n",
        "\n",
        "You will be looking at image classification in the first part of this assignment, the goal of image classification is to identify subjects within a given image. In the previous assignment, you looked at using MNIST, which is also a classification task \"which number is present\", where for images the gold standard is Imagent \"which class is present\".\n",
        "\n",
        "You can find out more information about Imagenet here:\n",
        "\n",
        "https://en.wikipedia.org/wiki/ImageNet\n",
        "\n",
        "\n",
        "Normally you would want to test classification on ImageNet as that's the dataset in which classification models tend to be trained on. However, the Imagenet dataset is not publicly available nor is it reasonable in size to download via Colab (100s of GBs).\n",
        "\n",
        "Instead, you will use the Caltech101 dataset. However, Caltech101 uses 101 labels which do not correspond to the Imagenet labels. As such, you will need to also download a bigger classification model to serve as a baseline for accuracy comparisons.\n",
        "\n",
        "More info can be found about the Caltech101 dataset here:\n",
        "\n",
        "https://en.wikipedia.org/wiki/Caltech_101\n",
        "\n",
        "Download the dataset you will be using: Caltech101"
      ],
      "metadata": {
        "id": "ntV9G5jFm-N2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to RGB class - some of the Caltech101 images are grayscale and do not match the tensor shapes\n",
        "class ConvertToRGB:\n",
        "    def __call__(self, image):\n",
        "        # If grayscale image, convert to RGB\n",
        "        if image.mode == \"L\":\n",
        "            image = Image.merge(\"RGB\", (image, image, image))\n",
        "        return image\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    ConvertToRGB(), # first convert to RGB\n",
        "    transforms.Resize((224, 224)),  # Most pretrained models expect 224x224 inputs\n",
        "    transforms.ToTensor(),\n",
        "    # this normalization is shared among all of the torch-hub models we will be using\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Download the dataset\n",
        "\n",
        "#!rm -rf ./data/caltech101/\n",
        "\n",
        "!wget -q \"https://data.caltech.edu/records/mzrjq-6wc02/files/caltech-101.zip\" -O caltech-101.zip\n",
        "!unzip -q -n caltech-101.zip -d ./data/caltech101\n",
        "\n",
        "!mv ./data/caltech101/caltech-101/* ./data/caltech101/ || true\n",
        "!rm -rf ./data/caltech101/caltech-101\n",
        "\n",
        "#!ls -R ./data/caltech101 | head -50\n",
        "\n",
        "# extract the images\n",
        "!tar -xzf ./data/caltech101/101_ObjectCategories.tar.gz -C ./data/caltech101/\n",
        "\n",
        "# extract the annotations\n",
        "!tar -xf ./data/caltech101/Annotations.tar -C ./data/caltech101/\n",
        "\n",
        "#!mv ./data/caltech101/caltech-101/* ./data/caltech101/ || true\n",
        "#!rm -rf ./data/caltech101/caltech-101\n",
        "\n",
        "!rm ./data/caltech101/*.gz\n",
        "!rm ./data/caltech101/*.tar\n",
        "\n",
        "#!ls -R ./data/caltech101 | head -50\n",
        "\n",
        "caltech101_dataset = datasets.Caltech101(root=\"./data\", download=False, transform=transform)"
      ],
      "metadata": {
        "id": "KZ2fe2Z3mLra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# set a manual seed for determinism\n",
        "torch.manual_seed(42)\n",
        "dataloader = DataLoader(caltech101_dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "e7ySkgqAnIAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the dataloader with a batch size of 16. You are fixing the seed for reproducibility."
      ],
      "metadata": {
        "id": "FmjKMANVnuk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download four classification models from torch-hub\n",
        "resnet152_model = torchvision.models.resnet152(pretrained=True)\n",
        "resnet50_model = torchvision.models.resnet50(pretrained=True)\n",
        "resnet18_model = torchvision.models.resnet18(pretrained=True)\n",
        "mobilenet_v2_model = torchvision.models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "# download a bigger classification model from huggingface to serve as a baseline\n",
        "vit_large_model = ViTForImageClassification.from_pretrained('google/vit-large-patch16-224')"
      ],
      "metadata": {
        "id": "DQpiOHbLn7Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Move the models to the GPU and set them in eval mode. This will disable dropout regularization and batch norm statistic calculation."
      ],
      "metadata": {
        "id": "bd4dpiUjKHNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet152_model = resnet152_model.to(\"cuda\").eval()\n",
        "resnet50_model = resnet50_model.to(\"cuda\").eval()\n",
        "resnet18_model = resnet18_model.to(\"cuda\").eval()\n",
        "mobilenet_v2_model = mobilenet_v2_model.to(\"cuda\").eval()\n",
        "vit_large_model = vit_large_model.to(\"cuda\").eval()"
      ],
      "metadata": {
        "id": "t0R-Tuf9oHjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download a series of models for testing. The VIT-L/16 model will serve as a baseline - this is a more accurate vision transformer based model.\n",
        "\n",
        "The other models you will use are:\n",
        "* resnet 18\n",
        "* resnet 50\n",
        "* resnet 152\n",
        "* mobilenet v2\n",
        "\n",
        "These are all different types of convolutional neural networks (CNNs), where ResNet adds a series of residual connections in the form: `out = x + block(x)`\n",
        "\n",
        "There's a good overview of the different versions here: https://towardsdatascience.com/understanding-and-visualizing-resnets-442284831be8\n",
        "\n",
        "MobileNet v2 is similar to ResNet, but introduces the idea of depth-wise convolutions and inverse bottleneck residual blocks. You will only be using it as a point of comparison, however, you can find out more details regarding the structure from here if interested: https://medium.com/@luis_gonzales/a-look-at-mobilenetv2-inverted-residuals-and-linear-bottlenecks-d49f85c12423"
      ],
      "metadata": {
        "id": "jcwp58f_JBsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, you will visualize the first image batch with their labels to make sure that the VIT-L/16 is working correctly. Luckily huggingface also implements an `id -> string` mapping, which will turn the classes into a human readable form."
      ],
      "metadata": {
        "id": "mq_wKs0KKTtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the first batch\n",
        "dataiter = iter(dataloader)\n",
        "images, _ = next(dataiter)\n",
        "\n",
        "# define a denorm helper function - this undoes the dataloader normalization so we can see the images better\n",
        "def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "    \"\"\" Denormalizes an image tensor that was previously normalized. \"\"\"\n",
        "    for t, m, s in zip(tensor, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "    return tensor\n",
        "\n",
        "# similarly, let's create an imshow helper function\n",
        "def imshow(tensor):\n",
        "    \"\"\" Display a tensor as an image. \"\"\"\n",
        "    tensor = tensor.permute(1, 2, 0)  # Change from C,H,W to H,W,C\n",
        "    tensor = denormalize(tensor)  # Denormalize if the tensor was normalized\n",
        "    tensor = tensor*0.24 + 0.5 # fix the image range, it still wasn't between 0 and 1\n",
        "    plt.imshow(tensor.clamp(0,1).cpu().numpy()) # plot the image\n",
        "    plt.axis('off')\n",
        "\n",
        "# for the actual code, we need to first predict the batch\n",
        "# we need to move the images to the GPU, and scale them by 0.5 because VIT-L/16 uses a different normalization to the other models\n",
        "with torch.no_grad(): # this isn't strictly needed since we already disabled autograd, but we should do it for good measure\n",
        "  output = vit_large_model(images.cuda()*0.5)\n",
        "\n",
        "# then we can sample the output using argmax (find the class with the highest probability)\n",
        "# here we are calling output.logits because huggingface returns a struct rather than a tuple\n",
        "# also, we apply argmax to the last dim (dim=-1) because that corresponds to the classes - the shape is B,C\n",
        "# and we also need to move the ids to the CPU from the GPU\n",
        "ids = output.logits.argmax(dim=-1).cpu()\n",
        "\n",
        "# next we will go through all of the ids and convert them into human readable labels\n",
        "# huggingface has the .config.id2label map, which helps.\n",
        "# notice that we are calling id.item() to get the raw contents of the ids tensor\n",
        "labels = []\n",
        "for id in ids:\n",
        "  labels += [vit_large_model.config.id2label[id.item()]]\n",
        "\n",
        "# finally, let's plot the first 4 images\n",
        "max_label_len = 25\n",
        "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        idx = i*4 + j\n",
        "        plt.sca(axes[i, j])\n",
        "        imshow(images[idx])\n",
        "        # we need to trim the labels because they sometimes are too long\n",
        "        if len(labels[idx]) > max_label_len:\n",
        "          trimmed_label = labels[idx][:max_label_len] + '...'\n",
        "        else:\n",
        "          trimmed_label = labels[idx]\n",
        "        axes[i,j].set_title(trimmed_label)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qF_VRyelKoy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1**\n",
        "\n",
        "Given the above classifications, how well do you think the model does?\n",
        "Can you observe any limitations? If so, do you think that's related to the model size and complexity, or is it more likely related to the training set?\n",
        "\n",
        "For more information, the class list can be found here: https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/\n",
        "\n",
        "Please answer below:"
      ],
      "metadata": {
        "id": "X_RG-jjMNdhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model does just \"okay\" at identifying each of the objects. The main limitation that I see is with identifying people. This could be due to the training set not having enough images of people in different scenairos to train the AI on."
      ],
      "metadata": {
        "id": "hyYnWGfDOFEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you're going to quantitatively measure the accuracy between the other models. The first thing you need to do is clear the GPU cache, to prevent an out-of-memory error. To undestand this, let's look at the current GPU memory utilization."
      ],
      "metadata": {
        "id": "cMiRH-_rOaXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run nvidia-smi to view the memory usage. Notice the ! before the command, this sends the command to the shell rather than python\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "NoQ1eG_8O0UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now you will manually invoke the python garbage collector using gc.collect()\n",
        "gc.collect()\n",
        "# and empty the GPU tensor cache - tensors that are no longer needed (activations essentially)\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "XihmXx6yOj8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run nvidia-smi again\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "8GoHcI_bPLsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you check above you should see the GPU memory utilization change from before and after the empty_cache() call. Memory management is one of the quirks that must be considered when dealing with accelerators like a GPU. Unlike with a CPU, there is no swap file to page memory in and out of the device. Instead, this must be handled by the user. When too much of the GPU memory is used, the driver will throw an out-of-memory error (commonly referred to as OOM). In this case, the process often ends up in an unrecoverable state and needs to be restarted to fully reset the memory utilization to zero.\n",
        "\n",
        "You should always try hard not to enter such a situation, as you then have to rerun the notebook from the first line.\n",
        "\n",
        "**Question 2**\n",
        "\n",
        "Given the above, why is the GPU memory utilization not zero? Does the current utilization match what you would expect? Please answer below:"
      ],
      "metadata": {
        "id": "FBx6r5LdPirM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because I called torch.cuda's empty_cache() function I utilized the GPU, the GPU also is most likely running some background functions in its idle state that takes up some memory. The memory usage dropped from 1901 MiB to down to 1715 MiB. This is not a massive change but it is still a difference."
      ],
      "metadata": {
        "id": "T11qdq0f82AE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the following helper function the compute the expected GPU memory utilization. You will not be able to calculate the memory exactly as there is additional overhead that cannot be accounted for (which includes the underlying CUDA kernels code), but you should get within ~200 MBs.\n",
        "\n",
        "**Question 3**\n",
        "\n",
        "In the cell below enter the code to estimate the current memory utilization:"
      ],
      "metadata": {
        "id": "hGYrY4iwQ6c5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to get element sizes in bytes\n",
        "def sizeof_tensor(tensor):\n",
        "    # Get the size of the data type\n",
        "    if (tensor.dtype == torch.float32) or (tensor.dtype == torch.float):      # float32 (single precision float)\n",
        "        bytes_per_element = 4\n",
        "    elif (tensor.dtype == torch.float16) or (tensor.dtype == torch.half):    # float16 (half precision float)\n",
        "        bytes_per_element = 2\n",
        "    else:\n",
        "      print(\"other dtype=\", tensor.dtype)\n",
        "    return bytes_per_element\n",
        "\n",
        "# helper function for counting parameters\n",
        "def count_parameters(model):\n",
        "  total_params = 0\n",
        "  for p in model.parameters():\n",
        "    total_params += p.numel()\n",
        "  return total_params\n",
        "\n",
        "# estimate the current GPU memory utilization\n",
        "def estimate_memory_utilization(model, dtype=torch.float32, activations=0):\n",
        "    param_size = sizeof_tensor(torch.zeros(1, dtype=dtype))\n",
        "    total_params = count_parameters(model)\n",
        "    params_memory = total_params * param_size / (1024 ** 2)\n",
        "    activations_memory = activations * param_size / (1024 ** 2)\n",
        "    return params_memory + activations_memory\n",
        "estimated_memory = estimate_memory_utilization(vit_large_model)\n",
        "print(str(estimated_memory) + \" MiB\" )"
      ],
      "metadata": {
        "id": "o4PW1y2nQ5Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have a better idea of what classification is doing for Imagenet, let's compare the accuracy for each of the downloaded models.\n",
        "You first need to reset the dataloader, and let's also change the batch size to improve GPU utilization."
      ],
      "metadata": {
        "id": "1GnIGDS9RByw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set a manual seed for determinism\n",
        "torch.manual_seed(42)\n",
        "dataloader = DataLoader(caltech101_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "sutUx5qSWwiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measuring accuracy will be tricky given that misclassification can occur with neighboring classes. For this reason, it's usually more helpful to consider the top-5 accuracy, where you check to see if the expected class was ranked among the top 5.\n",
        "As stated before, you will use the VIT-L/16 model as a baseline, and compare the top-1 class for VIT-L/16 with the top-5 of the other models."
      ],
      "metadata": {
        "id": "wG-sMrjNXwXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because this takes a while, let's only compute the first 10 batches. That should be enough to do some rough analysis. Since you are using a batch of 64, 10 batches are 640 images."
      ],
      "metadata": {
        "id": "mzS9al0zcPCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store results\n",
        "accuracies = {\"ResNet-18\": 0, \"ResNet-50\": 0, \"ResNet-152\": 0, \"MobileNetV2\": 0}\n",
        "total_samples = 0\n",
        "\n",
        "num_batches = len(dataloader)\n",
        "\n",
        "t_start = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, (inputs, _)in tqdm(enumerate(dataloader), desc=\"Processing batches\", total=num_batches):\n",
        "\n",
        "        if i > 10:\n",
        "          break\n",
        "\n",
        "        # move the inputs to the GPU\n",
        "        inputs = inputs.to(\"cuda\")\n",
        "\n",
        "        # Get top prediction from resnet152\n",
        "        #baseline_preds = resnet152_model(inputs).argmax(dim=1)\n",
        "        output = vit_large_model(inputs*0.5)\n",
        "        baseline_preds = output.logits.argmax(-1)\n",
        "\n",
        "        # ResNet-18 predictions\n",
        "        logits_resnet18 = resnet18_model(inputs)\n",
        "        top5_preds_resnet18 = logits_resnet18.topk(5, dim=1).indices\n",
        "        matches_resnet18 = (baseline_preds.unsqueeze(1) == top5_preds_resnet18).any(dim=1).float().sum().item()\n",
        "\n",
        "        # ResNet-50 predictions\n",
        "        logits_resnet50 = resnet50_model(inputs)\n",
        "        top5_preds_resnet50 = logits_resnet50.topk(5, dim=1).indices\n",
        "        matches_resnet50 = (baseline_preds.unsqueeze(1) == top5_preds_resnet50).any(dim=1).float().sum().item()\n",
        "\n",
        "        # ResNet-152 predictions\n",
        "        logits_resnet152 = resnet152_model(inputs)\n",
        "        top5_preds_resnet152 = logits_resnet152.topk(5, dim=1).indices\n",
        "        matches_resnet152 = (baseline_preds.unsqueeze(1) == top5_preds_resnet152).any(dim=1).float().sum().item()\n",
        "\n",
        "        # MobileNetV2 predictions\n",
        "        logits_mobilenetv2 = mobilenet_v2_model(inputs)\n",
        "        top5_preds_mobilenetv2 = logits_mobilenetv2.topk(5, dim=1).indices\n",
        "        matches_mobilenetv2 = (baseline_preds.unsqueeze(1) == top5_preds_mobilenetv2).any(dim=1).float().sum().item()\n",
        "\n",
        "        # Update accuracies\n",
        "        accuracies[\"ResNet-18\"] += matches_resnet18\n",
        "        accuracies[\"ResNet-50\"] += matches_resnet50\n",
        "        accuracies[\"ResNet-152\"] += matches_resnet152\n",
        "        accuracies[\"MobileNetV2\"] += matches_mobilenetv2\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "print()\n",
        "print(f\"took {time.time()-t_start}s\")\n",
        "\n",
        "# Finalize the accuracies\n",
        "accuracies[\"ResNet-18\"] /= total_samples\n",
        "accuracies[\"ResNet-50\"] /= total_samples\n",
        "accuracies[\"ResNet-152\"] /= total_samples\n",
        "accuracies[\"MobileNetV2\"] /= total_samples"
      ],
      "metadata": {
        "id": "KQ3M7Ta7pmm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4**\n",
        "\n",
        "In the cell below write the code to plot the accuracies for the different models using a bar graph."
      ],
      "metadata": {
        "id": "Bw0jbcWJaq2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the models and their accuracies\n",
        "models = list(accuracies.keys())\n",
        "accuracy_values = list(accuracies.values())\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(models, accuracy_values, color=['red', 'orange', 'green', 'purple'])\n",
        "\n",
        "\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Comparison of Models')\n",
        "\n",
        "for i, value in enumerate(accuracy_values):\n",
        "    plt.text(i, value + 0.01, f'{value:.2f}', ha='center', va='bottom')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FGVH-ynbrFn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that all of the models do decently, but some are better than others. Why is this and is there a quantifiable trend?\n",
        "\n",
        "**Question 5**\n",
        "\n",
        "To get a better understanding, let's compute the number of flops and parameters for each model based on a single image input. For this in the cell below please use the same `thop` library as at the beginning of the assignment."
      ],
      "metadata": {
        "id": "oWtujYE9a4lC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def profile(model):\n",
        "  input = torch.randn(1,3,224,224).cuda()\n",
        "\n",
        "  flops, params = thop.profile(model, inputs=(input, ), verbose=False)\n",
        "  print(f\"model {model.__class__.__name__} has {params:,} params and uses {flops:,} FLOPs\")\n",
        "  return flops, params\n"
      ],
      "metadata": {
        "id": "CE0qPwVlY76A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"ResNet-18\": resnet18_model,\n",
        "    \"ResNet-50\": resnet50_model,\n",
        "    \"ResNet-152\": resnet152_model,\n",
        "    \"MobileNetV2\": mobilenet_v2_model\n",
        "}\n",
        "flops_params = {}\n",
        "# Profiling each model\n",
        "for name, model in models.items():\n",
        "    flops, params = profile(model)\n",
        "    flops_params[name] = {\"flops\": flops, \"params\": params}\n",
        "\n",
        "# Extracting data for plotting\n",
        "model_names = list(flops_params.keys())\n",
        "flops_values = [flops_params[name][\"flops\"] for name in model_names]\n",
        "params_values = [flops_params[name][\"params\"] for name in model_names]\n",
        "accuracy_values = [accuracies[name] for name in model_names]  # Assuming accuracies is defined as before\n",
        "\n",
        "# Plot Accuracy vs Parameters\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(params_values, accuracy_values, color='blue')\n",
        "plt.title('Accuracy vs Parameters')\n",
        "plt.xlabel('Number of Parameters')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xscale('log')  # Log scale for better visualization\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot Accuracy vs FLOPs\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(flops_values, accuracy_values, color='green')\n",
        "plt.title('Accuracy vs FLOPs')\n",
        "plt.xlabel('Number of FLOPs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xscale('log')  # Log scale for better visualization\n",
        "plt.grid(True)\n",
        "\n",
        "# Show plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tp4FHt9SNA_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6**\n",
        "\n",
        "Do you notice any trends here? Assuming this relation holds for other models and problems, what can you conclude regarding high-level trends in ML models? Please enter your answer in the cell below:"
      ],
      "metadata": {
        "id": "w3bMmytEbqY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, the more parameters and FLOPs there are, the better the accuracy of the model."
      ],
      "metadata": {
        "id": "3eURxKPqfYzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "y0KVaKPSb8tA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance and Precision"
      ],
      "metadata": {
        "id": "h7dKq864c9BO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may have noticed that so far we have not been explicitly specifying the data types of these models. We can do this because torch will default to using float32 (32-bit single-precision).\n",
        "However, this is not always necessary nor desirable. There are currently a large number of alternative formats (with fewer bits per value), many of which are custom to specific accelerators. We will eventually cover these later in the course, but for now we can consider the second most common type on the GPU: FP16 (half-precision floating-point).\n",
        "\n",
        "As the name suggests, FP16 only uses 16 bits per value rather than 32. GPUs are specifically designed to handle this datatype and all of the newer ones can execute either one FP32 or two FP16 operations per ALU.\n",
        "\n",
        "Here's an overview of different precision types: https://moocaholic.medium.com/fp64-fp32-fp16-bfloat16-tf32-and-other-members-of-the-zoo-a1ca7897d407\n",
        "\n",
        "Modern GPUs support all of the ones listed, and many are supported by other accelerators like Google's TPU (the architecture that motivated bf16).\n",
        "\n",
        "You will start by converting the models to half precision, moving them back to the CPU, and then to the GPU again (this is needed to properly clear the caches)"
      ],
      "metadata": {
        "id": "zIm-R6i9dMbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the models to half\n",
        "resnet152_model = resnet152_model.half()\n",
        "resnet50_model = resnet50_model.half()\n",
        "resnet18_model = resnet18_model.half()\n",
        "mobilenet_v2_model = mobilenet_v2_model.half()\n",
        "vit_large_model = vit_large_model.half()\n",
        "\n",
        "# move them to the CPU\n",
        "resnet152_model = resnet152_model.cpu()\n",
        "resnet50_model = resnet50_model.cpu()\n",
        "resnet18_model = resnet18_model.cpu()\n",
        "mobilenet_v2_model = mobilenet_v2_model.cpu()\n",
        "vit_large_model = vit_large_model.cpu()\n",
        "\n",
        "# clean up the torch and CUDA state\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# move them back to the GPU\n",
        "resnet152_model = resnet152_model.cuda()\n",
        "resnet50_model = resnet50_model.cuda()\n",
        "resnet18_model = resnet18_model.cuda()\n",
        "mobilenet_v2_model = mobilenet_v2_model.cuda()\n",
        "vit_large_model = vit_large_model.cuda()"
      ],
      "metadata": {
        "id": "OxZrqqbwEjUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run nvidia-smi again\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "NAHjSgD_eM8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7**\n",
        "\n",
        "Now that the models are in half-precision, what do you notice about the memory utilization? Is the utilization what you would expect from your previous expected calculation given the new data types? Please answer below:"
      ],
      "metadata": {
        "id": "mtE1dvCFeksi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that there is a significant decrease in memory usage when switching to half-precision. The utilization is expected, it is about half of what it was originally."
      ],
      "metadata": {
        "id": "AaU7-PxDgA2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if inference is any faster now.\n",
        "First reset the data-loader like before."
      ],
      "metadata": {
        "id": "J-3mVGNyfLsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set a manual seed for determinism\n",
        "torch.manual_seed(42)\n",
        "dataloader = DataLoader(caltech101_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "jT8NCnEGeg4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And you can re-run the inference code. Notice that you also need to convert the inptus to .half()"
      ],
      "metadata": {
        "id": "eQ4ZZHDlfZk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store results\n",
        "accuracies = {\"ResNet-18\": 0, \"ResNet-50\": 0, \"ResNet-152\": 0, \"MobileNetV2\": 0}\n",
        "total_samples = 0\n",
        "\n",
        "num_batches = len(dataloader)\n",
        "\n",
        "t_start = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, (inputs, _)in tqdm(enumerate(dataloader), desc=\"Processing batches\", total=num_batches):\n",
        "\n",
        "        if i > 10:\n",
        "          break\n",
        "\n",
        "        # move the inputs to the GPU\n",
        "        inputs = inputs.to(\"cuda\").half()\n",
        "\n",
        "        # Get top prediction from resnet152\n",
        "        #baseline_preds = resnet152_model(inputs).argmax(dim=1)\n",
        "        output = vit_large_model(inputs*0.5)\n",
        "        baseline_preds = output.logits.argmax(-1)\n",
        "\n",
        "        # ResNet-18 predictions\n",
        "        logits_resnet18 = resnet18_model(inputs)\n",
        "        top5_preds_resnet18 = logits_resnet18.topk(5, dim=1).indices\n",
        "        matches_resnet18 = (baseline_preds.unsqueeze(1) == top5_preds_resnet18).any(dim=1).float().sum().item()\n",
        "\n",
        "        # ResNet-50 predictions\n",
        "        logits_resnet50 = resnet50_model(inputs)\n",
        "        top5_preds_resnet50 = logits_resnet50.topk(5, dim=1).indices\n",
        "        matches_resnet50 = (baseline_preds.unsqueeze(1) == top5_preds_resnet50).any(dim=1).float().sum().item()\n",
        "\n",
        "        # ResNet-152 predictions\n",
        "        logits_resnet152 = resnet152_model(inputs)\n",
        "        top5_preds_resnet152 = logits_resnet152.topk(5, dim=1).indices\n",
        "        matches_resnet152 = (baseline_preds.unsqueeze(1) == top5_preds_resnet152).any(dim=1).float().sum().item()\n",
        "\n",
        "        # MobileNetV2 predictions\n",
        "        logits_mobilenetv2 = mobilenet_v2_model(inputs)\n",
        "        top5_preds_mobilenetv2 = logits_mobilenetv2.topk(5, dim=1).indices\n",
        "        matches_mobilenetv2 = (baseline_preds.unsqueeze(1) == top5_preds_mobilenetv2).any(dim=1).float().sum().item()\n",
        "\n",
        "        # Update accuracies\n",
        "        accuracies[\"ResNet-18\"] += matches_resnet18\n",
        "        accuracies[\"ResNet-50\"] += matches_resnet50\n",
        "        accuracies[\"ResNet-152\"] += matches_resnet152\n",
        "        accuracies[\"MobileNetV2\"] += matches_mobilenetv2\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "print()\n",
        "print(f\"took {time.time()-t_start}s\")\n",
        "\n",
        "# Finalize the accuracies\n",
        "accuracies[\"ResNet-18\"] /= total_samples\n",
        "accuracies[\"ResNet-50\"] /= total_samples\n",
        "accuracies[\"ResNet-152\"] /= total_samples\n",
        "accuracies[\"MobileNetV2\"] /= total_samples"
      ],
      "metadata": {
        "id": "7QXaat_afWkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8**\n",
        "\n",
        "Did you observe a speedup? Was this result what you expected?\n",
        "What are the pros and cons to using a lower-precision format? Please answer below:"
      ],
      "metadata": {
        "id": "Am9lEKpFfw8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes there was a speed up, it was about three times faster than the original precision format. It is a little faster than I expected. The pros of using this lower precision format is better speed/throughput from the GPU and less memory used by the GPU. This has the negative effect however of worse precision and accuracy by the model."
      ],
      "metadata": {
        "id": "29pbtv1SgEUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9**\n",
        "\n",
        "Now that the inference is a bit faster, replot the bar graph with the accuracy for each model, along with the accuracy vs params and flops graph. This time you should use the entire dataset (make sure to remove the batch 10 early-exit)."
      ],
      "metadata": {
        "id": "w7K4akKygMJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies = {\"ResNet-18\": 0, \"ResNet-50\": 0, \"ResNet-152\": 0, \"MobileNetV2\": 0}\n",
        "total_samples = 0\n",
        "\n",
        "num_batches = len(dataloader)\n",
        "\n",
        "t_start = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, _) in tqdm(enumerate(dataloader), desc=\"Processing batches\", total=num_batches):\n",
        "        inputs = inputs.to(\"cuda\").half()\n",
        "\n",
        "        output = vit_large_model(inputs * 0.5)\n",
        "        baseline_preds = output.logits.argmax(-1)\n",
        "\n",
        "        logits_resnet18 = resnet18_model(inputs)\n",
        "        top5_preds_resnet18 = logits_resnet18.topk(5, dim=1).indices\n",
        "        matches_resnet18 = (baseline_preds.unsqueeze(1) == top5_preds_resnet18).any(dim=1).float().sum().item()\n",
        "\n",
        "        logits_resnet50 = resnet50_model(inputs)\n",
        "        top5_preds_resnet50 = logits_resnet50.topk(5, dim=1).indices\n",
        "        matches_resnet50 = (baseline_preds.unsqueeze(1) == top5_preds_resnet50).any(dim=1).float().sum().item()\n",
        "\n",
        "        logits_resnet152 = resnet152_model(inputs)\n",
        "        top5_preds_resnet152 = logits_resnet152.topk(5, dim=1).indices\n",
        "        matches_resnet152 = (baseline_preds.unsqueeze(1) == top5_preds_resnet152).any(dim=1).float().sum().item()\n",
        "\n",
        "        logits_mobilenetv2 = mobilenet_v2_model(inputs)\n",
        "        top5_preds_mobilenetv2 = logits_mobilenetv2.topk(5, dim=1).indices\n",
        "        matches_mobilenetv2 = (baseline_preds.unsqueeze(1) == top5_preds_mobilenetv2).any(dim=1).float().sum().item()\n",
        "\n",
        "        accuracies[\"ResNet-18\"] += matches_resnet18\n",
        "        accuracies[\"ResNet-50\"] += matches_resnet50\n",
        "        accuracies[\"ResNet-152\"] += matches_resnet152\n",
        "        accuracies[\"MobileNetV2\"] += matches_mobilenetv2\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "print()\n",
        "print(f\"took {time.time() - t_start}s\")\n",
        "accuracies[\"ResNet-18\"] /= total_samples\n",
        "accuracies[\"ResNet-50\"] /= total_samples\n",
        "accuracies[\"ResNet-152\"] /= total_samples\n",
        "accuracies[\"MobileNetV2\"] /= total_samples\n",
        "\n",
        "\n",
        "for model, accuracy in accuracies.items():\n",
        "    print(f\"{model} accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "UEoxzexrgDxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def profile(model):\n",
        "    input = torch.randn(1, 3, 224, 224).cuda().half()\n",
        "    flops, params = thop.profile(model, inputs=(input,), verbose=False)\n",
        "\n",
        "    return flops, params\n",
        "\n",
        "model_flops_params = {}\n",
        "for model in [resnet18_model, resnet50_model, resnet152_model, mobilenet_v2_model]:\n",
        "    flops, params = profile(model)\n",
        "    model_flops_params[model.__class__.__name__] = (flops, params)\n",
        "\n",
        "for model_name, (flops, params) in model_flops_params.items():\n",
        "    print(f\"{model_name}: {params:,} parameters, {flops:,} FLOPs\")\n",
        "\n"
      ],
      "metadata": {
        "id": "hiWppusDRjZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_flops_params_cleaned = {\n",
        "    'ResNet-18': model_flops_params['ResNet'],\n",
        "    'ResNet-50': model_flops_params['ResNet'],\n",
        "    'ResNet-152': model_flops_params['ResNet'],\n",
        "    'MobileNetV2': model_flops_params['MobileNetV2']\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(model_names, [accuracies[name] for name in model_names], color=['blue', 'green', 'orange', 'red'])\n",
        "plt.title('Model Accuracies')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(\n",
        "    [model_flops_params_cleaned[name][1] for name in model_names],\n",
        "    [accuracies[name] for name in model_names],\n",
        "    color='blue'\n",
        ")\n",
        "plt.title('Accuracy vs Parameters')\n",
        "plt.xlabel('Number of Parameters')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xscale('log')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(\n",
        "    [model_flops_params_cleaned[name][0] for name in model_names],\n",
        "    [accuracies[name] for name in model_names],\n",
        "    color='red'\n",
        ")\n",
        "plt.title('Accuracy vs FLOPs')\n",
        "plt.xlabel('FLOPs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xscale('log')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bKji6HNISCWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10**\n",
        "\n",
        "Do you notice any differences when comparing the full dataset to the batch 10 subset?"
      ],
      "metadata": {
        "id": "ngIUw92ggyf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "it takes much more time to process the full dataset compared to the 10 batch and the accuracy is lower across all models when processing the full dataset. The Resnet networks are also more accurate than the MobileNetV2 on the full data set which is different than the result seen on the batch 10 subset."
      ],
      "metadata": {
        "id": "GUY4Nrzsj17W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UEygIfNwg9F9"
      }
    }
  ]
}