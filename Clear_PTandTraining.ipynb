{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lee-53/ECE-4380-F25/blob/main/Clear_PTandTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch and Intro to Training"
      ],
      "metadata": {
        "id": "dcUHnV4tr3T-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start by importing necessary packages\n",
        "We will begin by importing necessary libraries for this notebook. Run the cell below to do so."
      ],
      "metadata": {
        "id": "qFR0_os7r97e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install thop\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import thop\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n"
      ],
      "metadata": {
        "id": "o-K6uCK8sAtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bW-BZIisdakj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking the torch version and CUDA access\n",
        "Let's start off by checking the current torch version, and whether we have CUDA availablity."
      ],
      "metadata": {
        "id": "zDPegMqisFcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"toch is using version:\", torch.__version__, \"with CUDA=\", torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdfzsLrPsLyl",
        "outputId": "daf35303-873d-4dd6-d4a4-60c2d2c61e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "toch is using version: 2.0.1+cu118 with CUDA= True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, we should see CUDA= False, meaning that the Colab instances does not have access to a GPU. To remedy this, click the drop-down arrow next to resources, and select \"Change Runtime Type\", then select \"T4 GPU\"\n",
        "Re-run the import cell, and the CUDA version / check above. It should now CUDA= True\n",
        "\n",
        "We won't be using the GPU just yet, but this prepares the instance for when we do."
      ],
      "metadata": {
        "id": "kWQIH1ybssV0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Brief Introduction to PyTorch\n",
        "\n",
        "PyTorch, or torch, is a machine learning framework developed my Facebook AI Research, which is competes with TensorFlow, JAX, and Caffe.\n",
        "\n",
        "Roughly speaking, these frameworks can be split into dynamic and static defintion frameworks.\n",
        "\n",
        "**Static Network Definition:** The architecture and computation flow are defined simultaneously. The order and manner in which data flows through the layers are fixed upon definition. These frameworks also tend to declare parameter shapes implicitly via the compute graph. This is typical of TensorFlow and JAX.\n",
        "\n",
        "**Dynamic Network Definition:** The architecture (layers/modules) is defined independently of the computation flow, often during the object's initialization. This allows for dynamic computation graphs where the flow of data can change during runtime based on conditions. Since the network exists independent of the compute graph, the parameter shapes must be declared explitly. PyTorch follows this approach.\n",
        "\n",
        "All of the ML frameworks support automatic differentiation, which is necessary to train a model (i.e. perform back propogation).\n",
        "\n",
        "Let's consider a typical pytorch module. These will inherit from the torch.nn.Module class, which provides many built in functions such as a wrapper for `__call__`, operations to move the module between devices (e.g. `cuda()`, `cpu()`), data-type conversion (e.g. `half()`, `float()`), and parameter and child management (e.g. `state_dict()`, `parameters()`)."
      ],
      "metadata": {
        "id": "ekG8rSD4tbpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inherit from torch.nn.Module\n",
        "class MyModule(nn.Module):\n",
        "  # constructor called upon creation\n",
        "  def __init__(self):\n",
        "    # the module has to initialize the parent first, which is what sets up the wrapper behavior\n",
        "    super().__init__()\n",
        "\n",
        "    # we can add sub-modules and parameters by assigning them to self\n",
        "    self.my_param = nn.Parameter(torch.zeros(4,8)) # this is how you define a raw parameter of shape 4x5\n",
        "    self.my_sub_module = nn.Linear(8,12)       # this is how you define a linear layer (tensorflow calls them Dense) of shape 8x12\n",
        "\n",
        "    # we can also add lists of modules, for example, the sequential layer\n",
        "    self.net = nn.Sequential(  # this layer type takes in a collection of modules rather than a list\n",
        "        nn.Linear(4,4),\n",
        "        nn.Linear(4,8),\n",
        "        nn.Linear(8,12)\n",
        "    )\n",
        "\n",
        "    # the above when calling self.net(x), will execute each module in the order they appear in a list\n",
        "    # it would be equivelent to x = self.net[2](self.net[1](self.net[0](x)))\n",
        "\n",
        "    # you can also create a list that doesn't execute\n",
        "    self.net_list = nn.ModuleList([\n",
        "        nn.Linear(7,7),\n",
        "        nn.Linear(7,9),\n",
        "        nn.Linear(9,14)\n",
        "    ])\n",
        "\n",
        "    # sometimes you will also see constant variables added to the module post init\n",
        "    foo = torch.Tensor([4])\n",
        "    self.register_buffer('foo', foo) # buffers allow .to(device, type) to apply\n",
        "\n",
        "  # let's define a forward function, which gets executed when calling the module, and defines the forward compute graph\n",
        "  def forward(self, x):\n",
        "\n",
        "    # if x is of shape Bx4\n",
        "    h1 =  x @ self.my_param # tensor-tensor multiplication uses the @ symbol\n",
        "    # then h1 is now shape Bx8, because my_param is 4x8... 2x4 * 4x8 = 2x8\n",
        "\n",
        "    h1 = self.my_sub_module(h1) # you execute a sub-module by calling it\n",
        "    # now, h1 is of shape Bx12, because my_sub_module was a 8x12 matrix\n",
        "\n",
        "    h2 = self.net(x)\n",
        "    # similarly, h2 is of shape Bx12, because that's the output of the sequence\n",
        "    # Bx4 -(4x4)-> Bx4 -(4x8)-> Bx8 -(8x12)-> Bx12\n",
        "\n",
        "    # since h1 and h2 are the same shape, they can be added together element-wise\n",
        "    return h1 + h2\n"
      ],
      "metadata": {
        "id": "lFxVVeLTsX7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can instantiate the module and perform a forward pass by calling it."
      ],
      "metadata": {
        "id": "gGVD-Rfy3YXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the module\n",
        "module = MyModule()\n",
        "\n",
        "# we can print the module to get a high-level summary of it\n",
        "print(\"=== printing the module ===\")\n",
        "print(module)\n",
        "print()\n",
        "# notice that the sub-module name is in parenthesis, and so are the list indicies\n",
        "\n",
        "# let's view the shape of one of the weight tensors\n",
        "print(\"my_sub_module weight tensor shape=\", module.my_sub_module.weight.shape)\n",
        "# the above works because nn.Linear has a member called .weight and .bias\n",
        "# to view the shape of my_param, you would use module.my_param\n",
        "# and to view the shape of the 2nd elment in net_list, you would use module.net_list[1].weight\n",
        "\n",
        "# we can iterate through all of the parameters via the state dict\n",
        "print()\n",
        "print(\"=== Listing parameters from the state_dict ===\")\n",
        "for key,value in module.state_dict().items():\n",
        "  print(f\"{key}: {value.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5SduF4H194T",
        "outputId": "89cb196f-68d3-4533-dfef-74e9d27862a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== printing the module ===\n",
            "MyModule(\n",
            "  (my_sub_module): Linear(in_features=8, out_features=12, bias=True)\n",
            "  (net): Sequential(\n",
            "    (0): Linear(in_features=4, out_features=4, bias=True)\n",
            "    (1): Linear(in_features=4, out_features=8, bias=True)\n",
            "    (2): Linear(in_features=8, out_features=12, bias=True)\n",
            "  )\n",
            "  (net_list): ModuleList(\n",
            "    (0): Linear(in_features=7, out_features=7, bias=True)\n",
            "    (1): Linear(in_features=7, out_features=9, bias=True)\n",
            "    (2): Linear(in_features=9, out_features=14, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "my_sub_module weight tensor shape= torch.Size([12, 8])\n",
            "\n",
            "=== Listing parameters from the state_dict ===\n",
            "my_param: torch.Size([4, 8])\n",
            "foo: torch.Size([1])\n",
            "my_sub_module.weight: torch.Size([12, 8])\n",
            "my_sub_module.bias: torch.Size([12])\n",
            "net.0.weight: torch.Size([4, 4])\n",
            "net.0.bias: torch.Size([4])\n",
            "net.1.weight: torch.Size([8, 4])\n",
            "net.1.bias: torch.Size([8])\n",
            "net.2.weight: torch.Size([12, 8])\n",
            "net.2.bias: torch.Size([12])\n",
            "net_list.0.weight: torch.Size([7, 7])\n",
            "net_list.0.bias: torch.Size([7])\n",
            "net_list.1.weight: torch.Size([9, 7])\n",
            "net_list.1.bias: torch.Size([9])\n",
            "net_list.2.weight: torch.Size([14, 9])\n",
            "net_list.2.bias: torch.Size([14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can perform a forward pass by first creating a tensor to send through\n",
        "x = torch.zeros(2,4)\n",
        "# then we call the module (this invokes MyModule.forward() )\n",
        "y = module(x)\n",
        "\n",
        "# then we can print the result and shape\n",
        "print(y, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZquyefY8r7W",
        "outputId": "d9e8c25d-3563-4970-c969-b30490b097a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.3488,  0.2606,  0.2858, -0.7264, -0.1835, -0.2437,  0.0204,  0.3403,\n",
            "          0.6845,  0.1302,  0.2239, -0.1460],\n",
            "        [-0.3488,  0.2606,  0.2858, -0.7264, -0.1835, -0.2437,  0.0204,  0.3403,\n",
            "          0.6845,  0.1302,  0.2239, -0.1460]], grad_fn=<AddBackward0>) torch.Size([2, 12])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few things to notice.\n",
        "\n",
        "\n",
        "1.   `x` was created with the shape 2x4, and in the forward pass, it gets manipulated into a 2x12 tensor. This the last dimension is explicit, while the first is called the batch dimmension, and only exists on data (a.k.a. activations). The output shape can be seen in the print statement from y.shape\n",
        "2.   You can view the shape of a tensor by using `.shape`, this is a very helpful trick for debugging tensor shape errors\n",
        "3.   In the output, there's a `grad_fn` component, this is the hook created by the forward trace to be used in back-propogation via automatic differentiation. The function name is `AddBackward`, because the last operation performed was `h1+h2`.\n",
        "\n",
        "We might not always want to trace the compute graph though, such as during inference. In such cases, we can use the `torch.no_grad()` context manager.\n"
      ],
      "metadata": {
        "id": "Iifk09KA3m7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we can perform a forward pass by first creating a tensor to send through\n",
        "x = torch.zeros(2,4)\n",
        "# then we call the module (this invokes MyModule.forward() )\n",
        "with torch.no_grad():\n",
        "  y = module(x)\n",
        "\n",
        "# then we can print the result and shape\n",
        "print(y, y.shape)\n",
        "# notice how the grad_fn is no longer part of the output tensor, that's because not_grad() disables the graph generation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AkENjSg-NDe",
        "outputId": "41311742-9a50-41ff-c401-21e2f514e070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0494, -0.4477,  0.2680,  0.5004, -0.1842,  0.2651,  0.1780, -0.1385,\n",
            "         -0.2703,  0.3432,  0.0133,  0.4349],\n",
            "        [-0.0494, -0.4477,  0.2680,  0.5004, -0.1842,  0.2651,  0.1780, -0.1385,\n",
            "         -0.2703,  0.3432,  0.0133,  0.4349]]) torch.Size([2, 12])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aside from passing a tensor through a model with the `no_grad()` context, we can also detach a tensor from the compute graph by calling `.detach()`. This will effectively make a copy of the original tensor, which allows it to be converted to numpy and visualized with matplotlib.\n",
        "\n",
        "**Note:** Tensors with a `grad_fn` property cannot be plotted and must first be detached."
      ],
      "metadata": {
        "id": "3kHh2aBOqxkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Layer-Perceptron (MLP) Prediction of MNIST\n",
        "\n",
        "With some basics out of the way, let's create a MLP for training MNIST.\n",
        "We can start by defining a simple torch model."
      ],
      "metadata": {
        "id": "t08BDXVur1z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    # define the constructor for the network\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # the input projection layer - projects into d=128\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        # the first hidden layer - compresses into d=64\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        # the final output layer - splits into 10 classes (digits 0-9)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    # define the forward pass compute graph\n",
        "    def forward(self, x):\n",
        "        # x is of shape BxHxW\n",
        "\n",
        "        # we first need to unroll the 2D image using view\n",
        "        # we set the first dim to be -1 meanining \"everything else\", the reason being that x is of shape BxHxW, where B is the batch dim\n",
        "        # we want to maintain different tensors for each training sample in the batch, which means the output should be of shape BxF where F is the feature dim\n",
        "        x = x.view(-1, 28*28)\n",
        "        # x is of shape Bx784\n",
        "\n",
        "        # project-in and apply a non-linearity (ReLU activation function)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        # x is of shape Bx128\n",
        "\n",
        "        # middle-layer and apply a non-linearity (ReLU activation function)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        # x is of shape Bx64\n",
        "\n",
        "        # project out into the 10 classes\n",
        "        x = self.fc3(x)\n",
        "        # x is of shape Bx10\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ud7V-wA0rPIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can begin training, we have to do a little boiler-plate to load the dataset. From the previous part, you saw how a hosted dataset can be loaded with TensorFlow. With pytorch it's a little more complicated, as we need to manually condition the input data."
      ],
      "metadata": {
        "id": "CyyFDqlbusSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a transformation for the input images. This uses torchvision.transforms, and .Compose will act similarly to nn.Sequential\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # first convert to a torch tensor\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # then normalize the input\n",
        "])\n",
        "\n",
        "# let's download the train and test datasets, applying the above transform - this will get saved locally into ./data, which is in the Colab instance\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "# we need to set the mini-batch (commonly referred to as \"batch\"), for now we can use 64\n",
        "batch_size = 64\n",
        "\n",
        "# then we need to create a dataloader for the train dataset, and we will also create one for the test dataset to evaluate performance\n",
        "# additionally, we will set the batch size in the dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# the torch dataloaders allow us to access the __getitem__ method, which returns a tuple of (data, label)\n",
        "# additionally, the dataloader will pre-colate the training samples into the given batch_size\n"
      ],
      "metadata": {
        "id": "nd_dVCuUuqY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the first element of the test_loader, and verify both the tensor shapes and data types. You can check the data-type with `.dtype`\n",
        "\n",
        "**Question 1**"
      ],
      "metadata": {
        "id": "lWtGZmUB5XcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first item\n",
        "first_item = next(iter(test_loader))\n",
        "\n",
        "# print out the element shapes, dtype, and identify which is the training sample and which is the training label - MNIST is a supervised learning task"
      ],
      "metadata": {
        "id": "PLB89iKQ5XBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the dataset loaded, we can instantiate the MLP model, the loss (or criteron function), and the optimizer for training."
      ],
      "metadata": {
        "id": "XW9dKZMl3050"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the model\n",
        "model = MLP()\n",
        "\n",
        "# we can print the model as well, but notice how the activation functions are missing. This is because they were called in the forward pass and not declared in the constructor\n",
        "print(model)\n",
        "\n",
        "# we can also count the model parameters\n",
        "param_count = sum([p.numel() for p in model.parameters()])\n",
        "print(f\"Model has {param_count:,} trainable parameters\")\n",
        "\n",
        "# for a critereon (loss) funciton, we will use Cross-Entropy Loss. This is the most common critereon used for multi-class prediction, and is also used by tokenized transformer models\n",
        "# it takes in an un-normalized probability distribution (i.e. without softmax) over N classes (in our case, 10 classes with MNIST). This distribution is then compared to an integer label\n",
        "# which is < N. For MNIST, the prediction might be [-0.0056, -0.2044,  1.1726,  0.0859,  1.8443, -0.9627,  0.9785, -1.0752, 1.1376,  1.8220], with the label 3.\n",
        "# Cross-entropy can be thought of as finding the difference between what the predicted distribution and the one-hot distribution\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# then we can intantiate the optimizer. We will use Stochastic Gradient Descent (SGD), and can set the learning rate to 0.1 with a momentum factor of 0.5\n",
        "# the first input to the optimizer is the list of model parameters, which is obtained by calling .parameters() on the model object\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZC7VVqm5ylL",
        "outputId": "262c86be-18f9-4c59-d6e4-08f49bae2cf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n",
            "Model has 109,386 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can define a training, and test loop"
      ],
      "metadata": {
        "id": "xsDmDIdt-ixB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an array to log the loss and accuracy\n",
        "train_losses = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "test_losses = []\n",
        "test_accuracy = []\n",
        "current_step = 0  # Start with global step 0\n",
        "current_epoch = 0 # Start with epoch 0"
      ],
      "metadata": {
        "id": "NxCih-WJJKXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# declare the train function\n",
        "def cpu_train(epoch, train_losses, steps, current_step):\n",
        "\n",
        "    # set the model in training mode - this doesn't do anything for us right now, but it is good practiced and needed with other layers such as batch norm and dropout\n",
        "    model.train()\n",
        "\n",
        "    # Create tqdm progress bar to help keep track of the training progress\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "\n",
        "    # loop over the dataset. Recall what comes out of the data loader, and then by wrapping that with enumerate() we get an index into the iterator list which we will call batch_idx\n",
        "    for batch_idx, (data, target) in pbar:\n",
        "\n",
        "        # during training, the first step is to zero all of the gradients through the optimizer\n",
        "        # this resets the state so that we can begin back propogation with the updated parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # then we apply a forward pass, which includes evaluating the loss (critereon)\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # given that we want to minimize the loss, we call .backward() on the result, which invokes the grad_fn property\n",
        "        loss.backward()\n",
        "\n",
        "        # the backward step will automatically differentiate the model and apply a gradient property to each of the parameters in the network\n",
        "        # so then all we have to do is call optimizer.step() to apply the gradients to the current parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # increment the step count\n",
        "        current_step += 1\n",
        "\n",
        "        # we should add some output to the progress bar so that we know which epoch we are training, and what the current loss is\n",
        "        if batch_idx % 100 == 0:\n",
        "\n",
        "            # append the last loss value\n",
        "            train_losses.append(loss.item())\n",
        "            steps.append(current_step)\n",
        "\n",
        "            desc = (f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}'\n",
        "                    f' ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "            pbar.set_description(desc)\n",
        "\n",
        "    return current_step\n",
        "\n",
        "# declare a test function, this will help us evaluate the model progress on a dataset which is different from the training dataset\n",
        "# doing so prevents cross-contamination and misleading results due to overfitting\n",
        "def cpu_test(test_losses, test_accuracy, steps, current_step):\n",
        "\n",
        "    # put the model into eval mode, this again does not currently do anything for us, but it is needed with other layers like batch_norm and dropout\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    # Create tqdm progress bar\n",
        "    pbar = tqdm(test_loader, total=len(test_loader), desc=\"Testing...\")\n",
        "\n",
        "    # since we are not training the model, and do not need back-propogation, we can use a no_grad() context\n",
        "    with torch.no_grad():\n",
        "        # iterate over the test set\n",
        "        for data, target in pbar:\n",
        "            # like with training, run a forward pass through the model and evaluate the critereon\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item() # we are using .item() to get the loss value rather than the tensor itself\n",
        "\n",
        "            # we can also check the accuracy by sampling the output - we can use greedy sampling which is argmax (maximum probability)\n",
        "            # in general, you would want to normalize the logits first (the un-normalized output of the model), which is done via .softmax()\n",
        "            # however, argmax is taking the maximum value, which will be the same index for the normalized and un-normalized distributions\n",
        "            # so we can skip a step and take argmax directly\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "\n",
        "    # append the final test loss\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracy.append(correct/len(test_loader.dataset))\n",
        "    steps.append(current_step)\n",
        "\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "          f' ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n"
      ],
      "metadata": {
        "id": "PdN0UYib8dBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train for 10 epochs\n",
        "for epoch in range(0, 10):\n",
        "    current_step = cpu_train(current_epoch, train_losses, train_steps, current_step)\n",
        "    cpu_test(test_losses, test_accuracy, test_steps, current_step)\n",
        "    current_epoch += 1"
      ],
      "metadata": {
        "id": "tp6R3sRhUu9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the loss for training and validation using matplotlib. They should be plotted on the same graph, labeled, and use a log-scale on the y-axis.\n",
        "\n",
        "**Question 2**"
      ],
      "metadata": {
        "id": "PapjPPn7HEDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the losses for the first 10 epochs\n"
      ],
      "metadata": {
        "id": "dRk-7A8L-xAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model may be able to train for a bit longer. Modify the previous training code to also report the time per epoch and the time for 10 epochs with testing. You can use `time.time()` to get the current time in seconds.\n",
        "Then run the model for another 10 epochs, printing out the execution time at the end, and replot the loss functions with the extra 10 epochs below.\n",
        "\n",
        "**Question 3**"
      ],
      "metadata": {
        "id": "KOeNII5-J2gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the losses for 20 epochs\n"
      ],
      "metadata": {
        "id": "dhI_yXAjKjzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make an observation from the above plot. Do the test and train loss curves indicate that the model should train longer to improve accuracy? Or does it indicate that 20 epochs is too long?\n",
        "\n",
        "**Question 4**"
      ],
      "metadata": {
        "id": "7LwAnncyLrv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nU8VwbfPMFT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Moving to the GPU\n",
        "\n",
        "Now that we have a model trained on the CPU, let's finally utilize the T4 GPU that we requested for this instance.\n",
        "\n",
        "using a GPU with torch is relatively simple, but has a few gotchas. Torch abstracts away most of the CUDA runtime API, but has a few hold-over concepts such as moving data between devices.\n",
        "Additionally, since the GPU is treated as a device seperate from the CPU, you cannot combine CPU and GPU based tensors in the same operation. Doing so will result in a device mismatch error. If this occurs, check where the tensors are located (you can always print `.device` on a tensor), and make sure they have been properly moved to the correct device.\n",
        "\n",
        "We'll start by creating a new model, optimizer, and critereon (not necessary but for completeness). However, one change that we'll make is moving the model to the GPU first. This can be done by calling `.cuda()` in general, or `.to(\"cuda\")` to be more explict. Specific GPU devices can be targetted such as `.to(\"cuda:0\")` for the first GPU (index 0), but we only have 1 GPU in colab so this is not necessary."
      ],
      "metadata": {
        "id": "uTlmnBdaMOCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the model\n",
        "model = MLP()\n",
        "\n",
        "# move the model to the GPU\n",
        "model.cuda()\n",
        "\n",
        "# for a critereon (loss) funciton, we will use Cross-Entropy Loss. This is the most common critereon used for multi-class prediction, and is also used by tokenized transformer models\n",
        "# it takes in an un-normalized probability distribution (i.e. without softmax) over N classes (in our case, 10 classes with MNIST). This distribution is then compared to an integer label\n",
        "# which is < N. For MNIST, the prediction might be [-0.0056, -0.2044,  1.1726,  0.0859,  1.8443, -0.9627,  0.9785, -1.0752, 1.1376,  1.8220], with the label 3.\n",
        "# Cross-entropy can be thought of as finding the difference between what the predicted distribution and the one-hot distribution\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# then we can intantiate the optimizer. We will use Stochastic Gradient Descent (SGD), and can set the learning rate to 0.1 with a momentum factor of 0.5\n",
        "# the first input to the optimizer is the list of model parameters, which is obtained by calling .parameters() on the model object\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
      ],
      "metadata": {
        "id": "wFzM3dX0LbTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new array to log the loss and accuracy\n",
        "train_losses = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "test_losses = []\n",
        "test_accuracy = []\n",
        "current_step = 0  # Start with global step 0\n",
        "current_epoch = 0 # Start with epoch 0"
      ],
      "metadata": {
        "id": "icc6zF4RN0aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, copy your previous training code with the timing parameters below.\n",
        "It needs to be slightly modified to move everything to the GPU.\n",
        "\n",
        "Before the line `output = model(data)`, add:\n",
        "```\n",
        "data = data.cuda()\n",
        "target = target.cuda()\n",
        "```\n",
        "\n",
        "Note that this is needed in both the train and test functions.\n",
        "\n",
        "**Question 5**\n"
      ],
      "metadata": {
        "id": "3GF2ZC05N4bA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the new GPU training functions\n"
      ],
      "metadata": {
        "id": "WfWk9TD8OSMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new GPU training for 10 epochs"
      ],
      "metadata": {
        "id": "LWQq5oIHZvem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is training faster now that it's on the GPU? Is the speedup what you would expect? Why or why not?\n",
        "\n",
        "**Question 6**"
      ],
      "metadata": {
        "id": "mmAxl15QP15i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eCW8zvzjP-_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Another Model Type: CNN\n",
        "\n",
        "We've trained a simple MLP for MNIST classification, however, MLPs are not a particularly good fit for images.\n",
        "\n",
        "Firstly, using a MLP will require that all images have the same size and shape, since they are unrolled in the input.\n",
        "\n",
        "Secondly, images can make use of translation invariance (a type of data symmetry), which cannot leveraged with a MLP.\n",
        "\n",
        "For these reasons, a convolutional network may be more appropriate, as it will pass kernels over the 2D image, removing the requirement for a fixed image size and leveraging the translation invariance of the 2D images.\n",
        "\n",
        "Let's define a simple CNN below."
      ],
      "metadata": {
        "id": "oY3b857_X13z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    # define the constructor for the network\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # instead of declaring the layers independently, let's use the nn.Sequential feature\n",
        "        # these blocks will be executed in list order\n",
        "\n",
        "        # we will break up the model into two parts:\n",
        "        # 1) the convolutional network\n",
        "        # 2) the prediction head (a small MLP)\n",
        "\n",
        "        # the convolutional network\n",
        "        self.net = nn.Sequential(\n",
        "          nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # the input projection layer - note that a sride of 1 means we are not down-sampling\n",
        "          nn.ReLU(),                                             # activation\n",
        "          nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), # an inner layer - note that a stride of 2 means we are down sampling. The output is 28x28 -> 14x14\n",
        "          nn.ReLU(),                                             # activation\n",
        "          nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),# an inner layer - note that a stride of 2 means we are down sampling. The output is 14x14 -> 7x7\n",
        "          nn.ReLU(),                                             # activation\n",
        "          nn.AdaptiveMaxPool2d(1),                               # a pooling layer which will output a 1x1 vector for the prediciton head\n",
        "        )\n",
        "\n",
        "        # the prediction head\n",
        "        self.head = nn.Sequential(\n",
        "          nn.Linear(128, 64),      # input projection, the output from the pool layer is a 128 element vector\n",
        "          nn.ReLU(),               # activation\n",
        "          nn.Linear(64, 10)        # class projection to one of the 10 classes (digits 0-9)\n",
        "        )\n",
        "\n",
        "\n",
        "    # define the forward pass compute graph\n",
        "    def forward(self, x):\n",
        "\n",
        "        # pass the input through the convolution network\n",
        "        x = self.net(x)\n",
        "\n",
        "        # reshape the output from Bx128x1x1 to Bx128\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # pass the pooled vector into the prediction head\n",
        "        x = self.head(x)\n",
        "\n",
        "        # the output here is Bx10\n",
        "        return x"
      ],
      "metadata": {
        "id": "qPrKiCT-QY4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the model\n",
        "model = CNN()\n",
        "\n",
        "# print the model and the parameter count\n",
        "print(model)\n",
        "param_count = sum([p.numel() for p in model.parameters()])\n",
        "print(f\"Model has {param_count:,} trainable parameters\")\n",
        "\n",
        "# the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# then we can intantiate the optimizer. We will use Stochastic Gradient Descent (SGD), and can set the learning rate to 0.1 with a momentum factor of 0.5\n",
        "# the first input to the optimizer is the list of model parameters, which is obtained by calling .parameters() on the model object\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nNkxodMTfs3",
        "outputId": "94f08ddf-5831-48eb-ed85-981d70d5fe00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (net): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): AdaptiveMaxPool2d(output_size=1)\n",
            "  )\n",
            "  (head): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Model has 101,578 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that this model now has fewer parameters than the MLP. Let's see how it trains.\n",
        "\n",
        "Using the previous code to train on the CPU with timing, execute 2 epochs of training.\n",
        "\n",
        "**Question 7**"
      ],
      "metadata": {
        "id": "yFTQhVJLcqFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new array to log the loss and accuracy\n",
        "train_losses = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "test_losses = []\n",
        "test_accuracy = []\n",
        "current_step = 0  # Start with global step 0\n",
        "current_epoch = 0 # Start with epoch 0"
      ],
      "metadata": {
        "id": "8lEsCyWFdBLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train for 2 epochs on the CPU"
      ],
      "metadata": {
        "id": "RNrJypFFdEWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's move the model to the GPU and try training for 2 epochs there.\n",
        "\n",
        "**Question 8**"
      ],
      "metadata": {
        "id": "KzTD8AHOdIlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the model\n",
        "model = CNN()\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "# print the model and the parameter count\n",
        "print(model)\n",
        "param_count = sum([p.numel() for p in model.parameters()])\n",
        "print(f\"Model has {param_count:,} trainable parameters\")\n",
        "\n",
        "# the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# then we can intantiate the optimizer. We will use Stochastic Gradient Descent (SGD), and can set the learning rate to 0.1 with a momentum factor of 0.5\n",
        "# the first input to the optimizer is the list of model parameters, which is obtained by calling .parameters() on the model object\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
      ],
      "metadata": {
        "id": "DUZs-CRbdTys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new array to log the loss and accuracy\n",
        "train_losses = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "test_losses = []\n",
        "test_accuracy = []\n",
        "current_step = 0  # Start with global step 0\n",
        "current_epoch = 0 # Start with epoch 0"
      ],
      "metadata": {
        "id": "_OBIhnMbdZX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train for 2 epochs on the GPU"
      ],
      "metadata": {
        "id": "Vz69D4h_dahU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do the CPU and GPU versions compare for the CNN? Is one faster than the other? Why do you think this is, and how does it differ from the MLP?\n",
        "\n",
        "**Question 9**"
      ],
      "metadata": {
        "id": "Nd3Lv1AKddG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GsZLGgvYdo2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a final comparison, we can profile the FLOPs (floating-point operations) executed by each model. We will use the thop.profile function for this and consider an MNIST batch size of 1."
      ],
      "metadata": {
        "id": "YPM7A8rNdqr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the input shape of a MNIST sample with batch_size = 1\n",
        "input = torch.randn(1, 1, 28, 28)\n",
        "\n",
        "# create a copy of the models on the CPU\n",
        "mlp_model = MLP()\n",
        "cnn_model = CNN()\n",
        "\n",
        "# profile the MLP\n",
        "flops, params = thop.profile(mlp_model, inputs=(input, ), verbose=False)\n",
        "print(f\"MLP has {params:,} params and uses {flops:,} FLOPs\")\n",
        "\n",
        "# profile the CNN\n",
        "flops, params = thop.profile(cnn_model, inputs=(input, ), verbose=False)\n",
        "print(f\"CNN has {params:,} params and uses {flops:,} FLOPs\")"
      ],
      "metadata": {
        "id": "H2PiALplYywV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are these results what you would have expected? Do they explain the performance difference between running on the CPU and GPU? Why or why not?\n",
        "\n",
        "**Question 10**"
      ],
      "metadata": {
        "id": "lrnbL1fIe0Eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uWnlxMVjfDUe"
      }
    }
  ]
}